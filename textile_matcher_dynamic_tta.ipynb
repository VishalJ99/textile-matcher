{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Textile Matching with TTA\n",
    "\n",
    "This notebook implements a production-ready textile matching system with:\n",
    "- Automatic ID assignment for any images dropped in source folder\n",
    "- Incremental embedding generation (only processes new images)\n",
    "- TTA (Test-Time Augmentation) for robust matching\n",
    "- Smart caching and mapping system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n# Set HuggingFace token from environment variable or prompt user\nif 'HF_TOKEN' not in os.environ:\n    print(\"‚ö†Ô∏è  HF_TOKEN not found in environment variables\")\n    print(\"Please set it using one of these methods:\")\n    print(\"  1. Export before running notebook: export HF_TOKEN='your_token'\")\n    print(\"  2. Create .env file with: HF_TOKEN=your_token\")\n    print(\"  3. Run: huggingface-cli login\")\n    # Optionally prompt for token (will not be saved in notebook)\n    # os.environ['HF_TOKEN'] = input(\"Enter HF token: \")\nelse:\n    print(\"‚úÖ HF_TOKEN found in environment\")\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nimport json\nimport hashlib\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport time\nfrom datetime import datetime\nimport shutil\n\n# DINOv3 imports\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Configuration\nBASE_DIR = Path(\"/workspace/textile_matching\")\nSOURCE_IMAGES_DIR = BASE_DIR / \"source_images\"  # Drop images here\nEMBEDDINGS_DIR = BASE_DIR / \"embeddings_tta\"    # TTA embeddings stored here\nMAPPINGS_DIR = BASE_DIR / \"mappings\"            # ID mappings stored here\nQUERIES_DIR = BASE_DIR / \"queries\"              # Query images\n\n# Create directories if they don't exist\nSOURCE_IMAGES_DIR.mkdir(exist_ok=True)\nEMBEDDINGS_DIR.mkdir(exist_ok=True)\nMAPPINGS_DIR.mkdir(exist_ok=True)\nQUERIES_DIR.mkdir(exist_ok=True)\n\nprint(f\"üìÅ Source images directory: {SOURCE_IMAGES_DIR}\")\nprint(f\"üìÅ Embeddings directory: {EMBEDDINGS_DIR}\")\nprint(f\"üìÅ Mappings directory: {MAPPINGS_DIR}\")\nprint(f\"üìÅ Queries directory: {QUERIES_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load DINOv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 567.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: facebook/dinov3-vitl16-pretrain-lvd1689m\n",
      "üñ•Ô∏è Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize DINOv3 model\n",
    "MODEL_NAME = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
    "\n",
    "print(\"Loading DINOv3 model...\")\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TTA Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dinov3_embedding_single(image, model, processor):\n",
    "    \"\"\"\n",
    "    Extract DINOv3 CLS token embedding from a PIL Image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", do_resize=False, do_center_crop=False)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    cls_token = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_token.cpu()\n",
    "\n",
    "\n",
    "def extract_dinov3_embedding_tta(image_path, model, processor, \n",
    "                                 rotations=[0, 90, 180, 270]):\n",
    "    \"\"\"\n",
    "    Extract DINOv3 embedding with Test-Time Augmentation (rotation).\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    embeddings = []\n",
    "    \n",
    "    for angle in rotations:\n",
    "        if angle != 0:\n",
    "            rotated_img = image.rotate(-angle, expand=True)\n",
    "        else:\n",
    "            rotated_img = image\n",
    "        \n",
    "        embedding = extract_dinov3_embedding_single(rotated_img, model, processor)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Average embeddings across all rotations\n",
    "    embeddings_tensor = torch.stack(embeddings)\n",
    "    averaged_embedding = embeddings_tensor.mean(dim=0)\n",
    "    \n",
    "    return averaged_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextileDatabase:\n",
    "    def __init__(self, source_dir, embeddings_dir, mappings_dir, model, processor):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.embeddings_dir = Path(embeddings_dir)\n",
    "        self.mappings_dir = Path(mappings_dir)\n",
    "        self.mapping_file = self.mappings_dir / \"image_mapping.json\"\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        \n",
    "        # Load or initialize mapping\n",
    "        self.mapping = self.load_mapping()\n",
    "        \n",
    "    def load_mapping(self):\n",
    "        \"\"\"Load existing mapping or create new one.\"\"\"\n",
    "        if self.mapping_file.exists():\n",
    "            with open(self.mapping_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {\"images\": {}, \"next_id\": 1}\n",
    "    \n",
    "    def save_mapping(self):\n",
    "        \"\"\"Save mapping to JSON file.\"\"\"\n",
    "        with open(self.mapping_file, 'w') as f:\n",
    "            json.dump(self.mapping, f, indent=2)\n",
    "    \n",
    "    def get_textile_id(self, image_path):\n",
    "        \"\"\"Get or assign textile ID for an image.\"\"\"\n",
    "        # Use relative path as key\n",
    "        rel_path = str(Path(image_path).relative_to(self.source_dir))\n",
    "        \n",
    "        if rel_path in self.mapping[\"images\"]:\n",
    "            return self.mapping[\"images\"][rel_path][\"id\"]\n",
    "        else:\n",
    "            # Assign new ID\n",
    "            textile_id = f\"textile_{self.mapping['next_id']:03d}\"\n",
    "            self.mapping[\"images\"][rel_path] = {\n",
    "                \"id\": textile_id,\n",
    "                \"original_name\": Path(image_path).name,\n",
    "                \"added_date\": datetime.now().isoformat(),\n",
    "                \"file_size\": Path(image_path).stat().st_size\n",
    "            }\n",
    "            self.mapping[\"next_id\"] += 1\n",
    "            self.save_mapping()\n",
    "            return textile_id\n",
    "    \n",
    "    def scan_and_update(self):\n",
    "        \"\"\"Scan source directory and update database.\"\"\"\n",
    "        # Get all images in source directory\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "        all_images = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            all_images.extend(self.source_dir.glob(f\"*{ext}\"))\n",
    "            all_images.extend(self.source_dir.glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        all_images = sorted(set(all_images))  # Remove duplicates and sort\n",
    "        \n",
    "        print(f\"\\nüì∑ Found {len(all_images)} images in source directory\")\n",
    "        \n",
    "        # Track new and existing\n",
    "        new_images = []\n",
    "        existing_images = []\n",
    "        \n",
    "        for img_path in all_images:\n",
    "            textile_id = self.get_textile_id(img_path)\n",
    "            embedding_path = self.embeddings_dir / f\"{textile_id}.pkl\"\n",
    "            \n",
    "            if embedding_path.exists():\n",
    "                existing_images.append((img_path, textile_id))\n",
    "            else:\n",
    "                new_images.append((img_path, textile_id))\n",
    "        \n",
    "        print(f\"‚úÖ {len(existing_images)} images already have embeddings\")\n",
    "        print(f\"üÜï {len(new_images)} new images need embeddings\")\n",
    "        \n",
    "        return new_images, existing_images\n",
    "    \n",
    "    def generate_embeddings_for_new_images(self, new_images):\n",
    "        \"\"\"Generate TTA embeddings only for new images.\"\"\"\n",
    "        if not new_images:\n",
    "            print(\"No new images to process.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüîÑ Generating TTA embeddings for {len(new_images)} new images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for img_path, textile_id in tqdm(new_images, desc=\"Processing\"):\n",
    "            # Generate TTA embedding\n",
    "            embedding = extract_dinov3_embedding_tta(img_path, self.model, self.processor)\n",
    "            \n",
    "            # Save embedding\n",
    "            embedding_path = self.embeddings_dir / f\"{textile_id}.pkl\"\n",
    "            with open(embedding_path, 'wb') as f:\n",
    "                pickle.dump(embedding, f)\n",
    "            \n",
    "            print(f\"  ‚úì {textile_id} <- {img_path.name}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ Completed in {elapsed:.1f}s ({elapsed/len(new_images):.2f}s per image)\")\n",
    "    \n",
    "    def load_all_embeddings(self):\n",
    "        \"\"\"Load all embeddings from disk.\"\"\"\n",
    "        embeddings = {}\n",
    "        \n",
    "        # Create reverse mapping from ID to original name\n",
    "        id_to_info = {}\n",
    "        for rel_path, info in self.mapping[\"images\"].items():\n",
    "            id_to_info[info[\"id\"]] = {\n",
    "                \"original_name\": info[\"original_name\"],\n",
    "                \"rel_path\": rel_path\n",
    "            }\n",
    "        \n",
    "        # Load embeddings\n",
    "        embedding_files = sorted(self.embeddings_dir.glob(\"*.pkl\"))\n",
    "        print(f\"\\nüì¶ Loading {len(embedding_files)} embeddings...\")\n",
    "        \n",
    "        for emb_path in tqdm(embedding_files):\n",
    "            textile_id = emb_path.stem\n",
    "            with open(emb_path, 'rb') as f:\n",
    "                embedding = pickle.load(f)\n",
    "            \n",
    "            # Store with both ID and original name for reference\n",
    "            if textile_id in id_to_info:\n",
    "                embeddings[textile_id] = {\n",
    "                    'embedding': embedding,\n",
    "                    'original_name': id_to_info[textile_id]['original_name'],\n",
    "                    'rel_path': id_to_info[textile_id]['rel_path']\n",
    "                }\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(embeddings)} embeddings\")\n",
    "        return embeddings\n",
    "    \n",
    "    def update_database(self):\n",
    "        \"\"\"Main function to update the database.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"DATABASE UPDATE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Scan for new images\n",
    "        new_images, existing_images = self.scan_and_update()\n",
    "        \n",
    "        # Generate embeddings for new images\n",
    "        if new_images:\n",
    "            self.generate_embeddings_for_new_images(new_images)\n",
    "        \n",
    "        # Load all embeddings\n",
    "        embeddings = self.load_all_embeddings()\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Database and Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database\n",
    "db = TextileDatabase(\n",
    "    source_dir=SOURCE_IMAGES_DIR,\n",
    "    embeddings_dir=EMBEDDINGS_DIR,\n",
    "    mappings_dir=MAPPINGS_DIR,\n",
    "    model=model,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# Update database (scan for new images and generate embeddings)\n",
    "database_embeddings = db.update_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(query_embedding, database_embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarities between query and database embeddings.\n",
    "    \"\"\"\n",
    "    query_norm = F.normalize(query_embedding.unsqueeze(0), p=2, dim=-1)\n",
    "    \n",
    "    similarities = []\n",
    "    for textile_id, data in database_embeddings.items():\n",
    "        db_embedding = data['embedding']\n",
    "        db_norm = F.normalize(db_embedding.unsqueeze(0), p=2, dim=-1)\n",
    "        \n",
    "        similarity = torch.matmul(query_norm, db_norm.T).item()\n",
    "        similarities.append({\n",
    "            'textile_id': textile_id,\n",
    "            'original_name': data['original_name'],\n",
    "            'similarity': similarity,\n",
    "            'rel_path': data['rel_path']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def search_textile(query_path, database_embeddings, model, processor, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar textiles using TTA query embedding.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Searching for: {query_path}\")\n",
    "    \n",
    "    # Extract TTA embedding for query\n",
    "    print(\"Extracting query embedding with TTA...\")\n",
    "    query_embedding = extract_dinov3_embedding_tta(query_path, model, processor)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = compute_similarities(query_embedding, database_embeddings)\n",
    "    \n",
    "    # Return top-k matches\n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Query Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify query image path\n",
    "# You can change this to any query image path\n",
    "QUERY_IMAGE = QUERIES_DIR / \"query_image.jpeg\"  # Change to \"query_2.png\" for second query\n",
    "\n",
    "# Alternative: directly specify path\n",
    "# QUERY_IMAGE = Path(\"/workspace/query_2.png\")\n",
    "\n",
    "if not QUERY_IMAGE.exists():\n",
    "    print(f\"‚ùå Query image not found: {QUERY_IMAGE}\")\n",
    "    print(f\"Available queries: {list(QUERIES_DIR.glob('*'))}\")\n",
    "else:\n",
    "    # Search for similar textiles\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEXTILE SEARCH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    matches = search_textile(\n",
    "        QUERY_IMAGE, \n",
    "        database_embeddings, \n",
    "        model, \n",
    "        processor, \n",
    "        top_k=10\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Top 10 Matches:\")\n",
    "    print(\"-\"*50)\n",
    "    for i, match in enumerate(matches, 1):\n",
    "        print(f\"{i:2d}. {match['textile_id']:12s} | Sim: {match['similarity']:.4f} | {match['original_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_search_results(query_path, matches, source_dir, top_k=5):\n",
    "    \"\"\"\n",
    "    Visualize query image and top matches.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    gs = gridspec.GridSpec(2, top_k+1, figure=fig, hspace=0.3, wspace=0.2)\n",
    "    \n",
    "    # Load and display query image\n",
    "    query_img = Image.open(query_path)\n",
    "    ax_query = fig.add_subplot(gs[:, 0])\n",
    "    ax_query.imshow(query_img)\n",
    "    ax_query.set_title(f\"Query Image\\n{Path(query_path).name}\", \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    ax_query.axis('off')\n",
    "    \n",
    "    # Display top matches\n",
    "    for idx, match in enumerate(matches[:top_k]):\n",
    "        # Load image\n",
    "        img_path = source_dir / match['rel_path']\n",
    "        if img_path.exists():\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Top row - image\n",
    "            ax_img = fig.add_subplot(gs[0, idx+1])\n",
    "            ax_img.imshow(img)\n",
    "            ax_img.set_title(f\"#{idx+1} {match['textile_id']}\\nSim: {match['similarity']:.3f}\", \n",
    "                            fontsize=10)\n",
    "            ax_img.axis('off')\n",
    "            \n",
    "            # Bottom row - filename\n",
    "            ax_text = fig.add_subplot(gs[1, idx+1])\n",
    "            ax_text.axis('off')\n",
    "            ax_text.text(0.5, 0.5, match['original_name'][:20], \n",
    "                        ha='center', va='center', fontsize=9, \n",
    "                        wrap=True, rotation=0)\n",
    "    \n",
    "    plt.suptitle(f\"Textile Search Results (TTA Embeddings)\", \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Visualize if we have matches\n",
    "if 'matches' in locals() and matches:\n",
    "    fig = visualize_search_results(\n",
    "        QUERY_IMAGE, \n",
    "        matches, \n",
    "        SOURCE_IMAGES_DIR, \n",
    "        top_k=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show database statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DATABASE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Total images in database: {len(database_embeddings)}\")\n",
    "print(f\"  ‚Ä¢ Embeddings directory: {EMBEDDINGS_DIR}\")\n",
    "print(f\"  ‚Ä¢ Source images directory: {SOURCE_IMAGES_DIR}\")\n",
    "print(f\"  ‚Ä¢ TTA rotations used: [0¬∞, 90¬∞, 180¬∞, 270¬∞]\")\n",
    "print(f\"  ‚Ä¢ Embedding dimension: {next(iter(database_embeddings.values()))['embedding'].shape[0] if database_embeddings else 'N/A'}\")\n",
    "\n",
    "# Show mapping info\n",
    "print(f\"\\nüìã Mapping Statistics:\")\n",
    "print(f\"  ‚Ä¢ Next available ID: textile_{db.mapping['next_id']:03d}\")\n",
    "print(f\"  ‚Ä¢ Mapping file: {db.mapping_file}\")\n",
    "\n",
    "# Show recent additions\n",
    "if db.mapping[\"images\"]:\n",
    "    print(f\"\\nüÜï Recently Added Images:\")\n",
    "    # Sort by date and show last 5\n",
    "    recent = sorted(\n",
    "        [(info[\"added_date\"], rel_path, info[\"id\"]) \n",
    "         for rel_path, info in db.mapping[\"images\"].items()],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    for date, rel_path, textile_id in recent:\n",
    "        print(f\"  ‚Ä¢ {textile_id}: {Path(rel_path).name} (added {date[:10]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Add New Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_image_to_database(image_path, db):\n",
    "    \"\"\"\n",
    "    Convenience function to add a single new image to the database.\n",
    "    \"\"\"\n",
    "    image_path = Path(image_path)\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        print(f\"‚ùå Image not found: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Copy to source directory if not already there\n",
    "    dest_path = db.source_dir / image_path.name\n",
    "    if not dest_path.exists():\n",
    "        shutil.copy2(image_path, dest_path)\n",
    "        print(f\"‚úÖ Copied {image_path.name} to source directory\")\n",
    "    \n",
    "    # Update database\n",
    "    print(\"Updating database...\")\n",
    "    updated_embeddings = db.update_database()\n",
    "    \n",
    "    return updated_embeddings\n",
    "\n",
    "# Example: Add new_dataset_case.jpeg if it exists\n",
    "new_image_path = Path(\"/workspace/new_dataset_case.jpeg\")\n",
    "if new_image_path.exists():\n",
    "    print(f\"\\nüÜï Adding new image: {new_image_path}\")\n",
    "    database_embeddings = add_new_image_to_database(new_image_path, db)\n",
    "else:\n",
    "    print(f\"\\nüí° To add a new image, place it in: {SOURCE_IMAGES_DIR}\")\n",
    "    print(f\"   Then run: database_embeddings = db.update_database()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test with Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test multiple queries\n",
    "def test_multiple_queries(query_paths, database_embeddings, model, processor):\n",
    "    \"\"\"\n",
    "    Test multiple query images and show results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for query_path in query_paths:\n",
    "        query_path = Path(query_path)\n",
    "        if query_path.exists():\n",
    "            print(f\"\\nüîç Testing query: {query_path.name}\")\n",
    "            matches = search_textile(query_path, database_embeddings, model, processor, top_k=3)\n",
    "            \n",
    "            print(f\"Top 3 matches:\")\n",
    "            for i, match in enumerate(matches, 1):\n",
    "                print(f\"  {i}. {match['textile_id']}: {match['similarity']:.4f} ({match['original_name']})\")\n",
    "            \n",
    "            results[query_path.name] = matches\n",
    "        else:\n",
    "            print(f\"‚ùå Query not found: {query_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# List of queries to test\n",
    "test_queries = [\n",
    "    QUERIES_DIR / \"query_image.jpeg\",\n",
    "    QUERIES_DIR / \"query_2.png\",\n",
    "    # Add more query paths as needed\n",
    "]\n",
    "\n",
    "# Filter to only existing files\n",
    "existing_queries = [q for q in test_queries if q.exists()]\n",
    "\n",
    "if existing_queries:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MULTIPLE QUERY TEST\")\n",
    "    print(\"=\"*60)\n",
    "    test_results = test_multiple_queries(existing_queries, database_embeddings, model, processor)\n",
    "else:\n",
    "    print(\"No test queries found. Add images to:\", QUERIES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Instructions for Use\n",
    "\n",
    "### How to use this notebook:\n",
    "\n",
    "1. **Add reference images**: Drop any textile images into `/workspace/textile_matching/source_images/`\n",
    "   - Any filename is fine - the system assigns IDs automatically\n",
    "   - Images are never renamed, just tracked\n",
    "\n",
    "2. **Run the notebook**: It will:\n",
    "   - Detect new images automatically\n",
    "   - Generate TTA embeddings only for new images\n",
    "   - Load existing embeddings from cache\n",
    "\n",
    "3. **Search with a query**: \n",
    "   - Change `QUERY_IMAGE` in section 7 to your query path\n",
    "   - Or use the `search_textile()` function directly\n",
    "\n",
    "4. **Add new images anytime**:\n",
    "   - Drop new images in source folder\n",
    "   - Run `database_embeddings = db.update_database()`\n",
    "   - Only new images are processed\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ Incremental updates (only process new images)\n",
    "- ‚úÖ TTA for robust matching\n",
    "- ‚úÖ Persistent ID mapping\n",
    "- ‚úÖ Original filenames preserved\n",
    "- ‚úÖ Fast loading from cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}